---
title: Multi-Broker Workflows
description: Patterns for routing events across multiple message brokers.
author: Herald Team
published: 2025-12-06
tags: [Cookbook, Multi-Broker, Patterns]
---

# Multi-Broker Workflows

Patterns for routing events across different message brokers.

## Kafka to SQS

Consume from Kafka, process, publish to SQS:

```go
package main

import (
    "context"
    kafkago "github.com/segmentio/kafka-go"
    "github.com/zoobzio/capitan"
    "github.com/zoobzio/herald"
    "github.com/zoobzio/herald/kafka"
    "github.com/zoobzio/herald/sqs"
)

type Order struct {
    ID    string  `json:"id"`
    Total float64 `json:"total"`
}

type Fulfillment struct {
    OrderID string `json:"order_id"`
    Status  string `json:"status"`
}

var (
    orderReceived      = capitan.NewSignal("order.received", "Order from Kafka")
    fulfillmentCreated = capitan.NewSignal("fulfillment.created", "Fulfillment for SQS")
    orderKey           = capitan.NewKey[Order]("order", "app.Order")
    fulfillmentKey     = capitan.NewKey[Fulfillment]("fulfillment", "app.Fulfillment")
)

func main() {
    ctx := context.Background()

    // Kafka consumer - create kafka-go reader
    kafkaReader := kafkago.NewReader(kafkago.ReaderConfig{
        Brokers: []string{"localhost:9092"},
        Topic:   "orders",
        GroupID: "fulfillment-service",
    })
    defer kafkaReader.Close()

    kafkaProvider := kafka.New("orders", kafka.WithReader(kafkaReader))
    defer kafkaProvider.Close()

    orderSub := herald.NewSubscriber(kafkaProvider, orderReceived, orderKey, nil)
    orderSub.Start(ctx)
    defer orderSub.Close()

    // SQS publisher - create SQS client implementing sqs.Client interface
    sqsClient := createSQSClient() // Your AWS SDK wrapper
    sqsProvider := sqs.New("https://sqs.us-east-1.amazonaws.com/123/fulfillments",
        sqs.WithClient(sqsClient),
    )
    defer sqsProvider.Close()

    fulfillmentPub := herald.NewPublisher(sqsProvider, fulfillmentCreated, fulfillmentKey, nil)
    fulfillmentPub.Start(ctx)
    defer fulfillmentPub.Close()

    // Bridge logic
    capitan.Hook(orderReceived, func(ctx context.Context, e *capitan.Event) {
        order, ok := orderKey.From(e)
        if !ok {
            return
        }

        // Create fulfillment
        fulfillment := Fulfillment{
            OrderID: order.ID,
            Status:  "pending",
        }

        // Propagate metadata
        meta := herald.MetadataFromContext(ctx)
        ctx = herald.ContextWithMetadata(ctx, meta)

        capitan.Emit(ctx, fulfillmentCreated, fulfillmentKey.Field(fulfillment))
    })

    // Run until interrupted
    select {}
}
```

## Fan-Out to Multiple Brokers

Publish the same event to multiple destinations:

```go
package main

import (
    "context"
    kafkago "github.com/segmentio/kafka-go"
    "github.com/zoobzio/capitan"
    "github.com/zoobzio/herald"
    "github.com/zoobzio/herald/kafka"
    "github.com/zoobzio/herald/sqs"
    "github.com/zoobzio/herald/pubsub"
)

type Event struct {
    Type    string `json:"type"`
    Payload string `json:"payload"`
}

var (
    eventCreated = capitan.NewSignal("event.created", "Event to fan out")
    eventKey     = capitan.NewKey[Event]("event", "app.Event")
)

func main() {
    ctx := context.Background()

    // Create publishers for each destination

    // Kafka - create writer
    kafkaWriter := &kafkago.Writer{
        Addr:  kafkago.TCP("localhost:9092"),
        Topic: "events",
    }
    defer kafkaWriter.Close()
    kafkaProvider := kafka.New("events", kafka.WithWriter(kafkaWriter))
    defer kafkaProvider.Close()

    // SQS - create client
    sqsClient := createSQSClient()
    sqsProvider := sqs.New("https://sqs.us-east-1.amazonaws.com/123/events",
        sqs.WithClient(sqsClient),
    )
    defer sqsProvider.Close()

    // Pub/Sub - create client
    pubsubClient := createPubSubClient()
    pubsubProvider := pubsub.New("events-topic",
        pubsub.WithPublisher(pubsubClient),
    )
    defer pubsubProvider.Close()

    // Start all publishers on same signal
    kafkaPub := herald.NewPublisher(kafkaProvider, eventCreated, eventKey, nil)
    kafkaPub.Start(ctx)
    defer kafkaPub.Close()

    sqsPub := herald.NewPublisher(sqsProvider, eventCreated, eventKey, nil)
    sqsPub.Start(ctx)
    defer sqsPub.Close()

    pubsubPub := herald.NewPublisher(pubsubProvider, eventCreated, eventKey, nil)
    pubsubPub.Start(ctx)
    defer pubsubPub.Close()

    // Single emit goes to all three brokers
    capitan.Emit(ctx, eventCreated, eventKey.Field(Event{
        Type:    "user.created",
        Payload: `{"id":"123"}`,
    }))

    capitan.Shutdown()
}
```

## Conditional Routing

Route to different brokers based on content:

```go
package main

import (
    "context"
    kafkago "github.com/segmentio/kafka-go"
    "github.com/zoobzio/capitan"
    "github.com/zoobzio/herald"
    "github.com/zoobzio/herald/kafka"
    "github.com/zoobzio/herald/sqs"
)

type Order struct {
    ID       string  `json:"id"`
    Total    float64 `json:"total"`
    Priority string  `json:"priority"`
}

var (
    orderReceived     = capitan.NewSignal("order.received", "Order received")
    highPriorityOrder = capitan.NewSignal("order.high_priority", "High priority order")
    normalOrder       = capitan.NewSignal("order.normal", "Normal priority order")
    orderKey          = capitan.NewKey[Order]("order", "app.Order")
)

func main() {
    ctx := context.Background()

    // Input: all orders - create kafka-go reader
    kafkaReader := kafkago.NewReader(kafkago.ReaderConfig{
        Brokers: []string{"localhost:9092"},
        Topic:   "orders",
        GroupID: "order-router",
    })
    defer kafkaReader.Close()

    kafkaProvider := kafka.New("orders", kafka.WithReader(kafkaReader))
    defer kafkaProvider.Close()

    orderSub := herald.NewSubscriber(kafkaProvider, orderReceived, orderKey, nil)
    orderSub.Start(ctx)
    defer orderSub.Close()

    // Output: high priority → SQS
    sqsClient := createSQSClient()
    highPriorityProvider := sqs.New("https://sqs.../high-priority-orders",
        sqs.WithClient(sqsClient),
    )
    defer highPriorityProvider.Close()

    // Output: normal → Kafka
    normalWriter := &kafkago.Writer{
        Addr:  kafkago.TCP("localhost:9092"),
        Topic: "normal-orders",
    }
    defer normalWriter.Close()
    normalProvider := kafka.New("normal-orders", kafka.WithWriter(normalWriter))
    defer normalProvider.Close()

    highPub := herald.NewPublisher(highPriorityProvider, highPriorityOrder, orderKey, nil)
    highPub.Start(ctx)
    defer highPub.Close()

    normalPub := herald.NewPublisher(normalProvider, normalOrder, orderKey, nil)
    normalPub.Start(ctx)
    defer normalPub.Close()

    // Routing logic
    capitan.Hook(orderReceived, func(ctx context.Context, e *capitan.Event) {
        order, ok := orderKey.From(e)
        if !ok {
            return
        }

        // Route based on priority
        if order.Priority == "high" || order.Total > 1000 {
            capitan.Emit(ctx, highPriorityOrder, orderKey.Field(order))
        } else {
            capitan.Emit(ctx, normalOrder, orderKey.Field(order))
        }
    })

    select {}
}
```

## Broker Migration

Migrate from one broker to another with dual-write:

```go
package main

import (
    "context"
    "os"
    kafkago "github.com/segmentio/kafka-go"
    "github.com/zoobzio/capitan"
    "github.com/zoobzio/herald"
    "github.com/zoobzio/herald/kafka"
    "github.com/zoobzio/herald/pubsub"
)

type Order struct {
    ID string `json:"id"`
}

var (
    orderCreated = capitan.NewSignal("order.created", "Order created")
    orderKey     = capitan.NewKey[Order]("order", "app.Order")
)

func main() {
    ctx := context.Background()

    // Old broker (Kafka) - create writer
    kafkaWriter := &kafkago.Writer{
        Addr:  kafkago.TCP("localhost:9092"),
        Topic: "orders",
    }
    defer kafkaWriter.Close()

    kafkaProvider := kafka.New("orders", kafka.WithWriter(kafkaWriter))
    defer kafkaProvider.Close()

    kafkaPub := herald.NewPublisher(kafkaProvider, orderCreated, orderKey, nil)
    kafkaPub.Start(ctx)
    defer kafkaPub.Close()

    // New broker (Pub/Sub) - enabled via feature flag
    if os.Getenv("ENABLE_PUBSUB") == "true" {
        pubsubClient := createPubSubClient()
        pubsubProvider := pubsub.New("orders", pubsub.WithPublisher(pubsubClient))
        defer pubsubProvider.Close()

        pubsubPub := herald.NewPublisher(pubsubProvider, orderCreated, orderKey, nil)
        pubsubPub.Start(ctx)
        defer pubsubPub.Close()
    }

    // Application publishes normally - goes to both during migration
    capitan.Emit(ctx, orderCreated, orderKey.Field(Order{ID: "123"}))

    capitan.Shutdown()
}
```

### Migration Phases

```
Phase 1: Dual-Write
─────────────────────
    orderCreated ──► Kafka (primary)
                 └─► Pub/Sub (shadow)

    Consumers read from Kafka only

Phase 2: Dual-Read
─────────────────────
    orderCreated ──► Kafka
                 └─► Pub/Sub

    Some consumers migrate to Pub/Sub

Phase 3: Cutover
─────────────────────
    orderCreated ──► Pub/Sub (primary)
                 └─► Kafka (shadow, then removed)

    All consumers on Pub/Sub
```

## Aggregator Pattern

Aggregate from multiple sources:

```go
package main

import (
    "context"
    kafkago "github.com/segmentio/kafka-go"
    "github.com/zoobzio/capitan"
    "github.com/zoobzio/herald"
    "github.com/zoobzio/herald/kafka"
    "github.com/zoobzio/herald/sqs"
)

type Event struct {
    Source string `json:"source"`
    Data   string `json:"data"`
}

var (
    kafkaEvent      = capitan.NewSignal("event.kafka", "Event from Kafka")
    sqsEvent        = capitan.NewSignal("event.sqs", "Event from SQS")
    aggregatedEvent = capitan.NewSignal("event.aggregated", "Aggregated event")
    eventKey        = capitan.NewKey[Event]("event", "app.Event")
)

func main() {
    ctx := context.Background()

    // Subscribe from Kafka - create reader
    kafkaReader := kafkago.NewReader(kafkago.ReaderConfig{
        Brokers: []string{"localhost:9092"},
        Topic:   "events",
        GroupID: "aggregator",
    })
    defer kafkaReader.Close()

    kafkaProvider := kafka.New("events", kafka.WithReader(kafkaReader))
    defer kafkaProvider.Close()

    kafkaSub := herald.NewSubscriber(kafkaProvider, kafkaEvent, eventKey, nil)
    kafkaSub.Start(ctx)
    defer kafkaSub.Close()

    // Subscribe from SQS - create client
    sqsClient := createSQSClient()
    sqsProvider := sqs.New("https://sqs.../events", sqs.WithClient(sqsClient))
    defer sqsProvider.Close()

    sqsSub := herald.NewSubscriber(sqsProvider, sqsEvent, eventKey, nil)
    sqsSub.Start(ctx)
    defer sqsSub.Close()

    // Aggregate to single stream
    aggregator := func(source string) func(context.Context, *capitan.Event) {
        return func(ctx context.Context, e *capitan.Event) {
            event, ok := eventKey.From(e)
            if !ok {
                return
            }
            event.Source = source
            capitan.Emit(ctx, aggregatedEvent, eventKey.Field(event))
        }
    }

    capitan.Hook(kafkaEvent, aggregator("kafka"))
    capitan.Hook(sqsEvent, aggregator("sqs"))

    // Process aggregated stream
    capitan.Hook(aggregatedEvent, func(ctx context.Context, e *capitan.Event) {
        event, _ := eventKey.From(e)
        processEvent(event)
    })

    select {}
}

func processEvent(e Event) {
    // Unified processing for all sources
}
```

## Metadata Preservation

Ensure metadata flows across broker boundaries:

```go
capitan.Hook(orderReceived, func(ctx context.Context, e *capitan.Event) {
    order, ok := orderKey.From(e)
    if !ok {
        return
    }

    // Get incoming metadata
    inMeta := herald.MetadataFromContext(ctx)

    // Preserve and extend
    outMeta := herald.Metadata{
        "correlation-id": inMeta["correlation-id"],
        "original-source": inMeta["source"],
        "processed-by":    "fulfillment-service",
    }

    ctx = herald.ContextWithMetadata(ctx, outMeta)
    capitan.Emit(ctx, fulfillmentCreated, fulfillmentKey.Field(fulfillment))
})
```

## Best Practices

### 1. Use Distinct Signal Names

```go
// ✅ Clear naming
orderFromKafka   = capitan.NewSignal("order.kafka.received", "...")
orderFromSQS     = capitan.NewSignal("order.sqs.received", "...")
orderToWarehouse = capitan.NewSignal("order.warehouse.created", "...")

// ❌ Ambiguous
orderReceived = capitan.NewSignal("order.received", "...")
```

### 2. Handle Failures Per-Broker

```go
kafkaOpts := []herald.Option[Order]{
    herald.WithBackoff[Order](5, 1*time.Second), // Kafka can be slow
}

sqsOpts := []herald.Option[Order]{
    herald.WithTimeout[Order](30 * time.Second), // SQS has longer latency
}
```

### 3. Monitor Each Leg

```go
// Track per-broker metrics
capitan.Hook(orderFromKafka, func(ctx context.Context, e *capitan.Event) {
    metrics.Inc("orders_received", "source", "kafka")
    // ...
})

capitan.Hook(orderToSQS, func(ctx context.Context, e *capitan.Event) {
    metrics.Inc("orders_published", "destination", "sqs")
    // ...
})
```

## See Also

- [Error Handling](./3.error-handling.md) - Handle failures in multi-broker flows
- [Providers Guide](../3.guides/2.providers.md) - Provider configuration
- [Metadata Guide](../3.guides/4.metadata.md) - Header propagation
