---
title: Error Handling
description: Strategies for handling failures in herald integrations.
author: Herald Team
published: 2025-12-06
tags: [Cookbook, Error Handling, Patterns]
---

# Error Handling

Strategies for handling failures in herald integrations.

## Debugging with Error Signal

Herald emits `herald.ErrorSignal` when operations fail. Hook into it for visibility:

```go
capitan.Hook(herald.ErrorSignal, func(ctx context.Context, e *capitan.Event) {
    err, ok := herald.ErrorKey.From(e)
    if !ok {
        return
    }

    log.Error("Herald error",
        "operation", err.Operation,  // "publish", "subscribe", "unmarshal"
        "signal", err.Signal,        // your signal name
        "error", err.Err,
        "nack", err.Nack,            // true if message was nack'd
    )

    // For unmarshal errors, raw bytes are available
    if err.Raw != nil {
        log.Debug("Raw message", "data", string(err.Raw))
    }
})
```

This captures:
- Provider errors (connection issues, broker unavailable)
- Unmarshal errors (invalid JSON, type mismatches)
- Pipeline errors (timeout, circuit breaker open)
- Publish failures (broker rejected message)

## Retry with Backoff

Use `WithBackoff` for transient failures:

```go
opts := []herald.Option[Order]{
    herald.WithBackoff[Order](3, 500*time.Millisecond),
}

pub := herald.NewPublisher(provider, signal, key, opts)
```

### Retry Behavior

```
Attempt 1 ──[fail]──► wait 500ms
                           │
Attempt 2 ──[fail]──► wait 1000ms
                           │
Attempt 3 ──[fail]──► Return error (Nack for subscribers)
           or
Attempt 2 ──[success]──► Return success (Ack for subscribers)
```

### Combined with Timeout

```go
opts := []herald.Option[Order]{
    // Total operation must complete in 10s (including all retries)
    herald.WithTimeout[Order](10 * time.Second),
    // Retry up to 3 times
    herald.WithBackoff[Order](3, 500*time.Millisecond),
}
```

## Dead Letter Queue Simulation

Herald doesn't have built-in DLQ, but you can implement it:

```go
package main

import (
    "context"
    "sync/atomic"
    kafkago "github.com/segmentio/kafka-go"
    "github.com/zoobzio/capitan"
    "github.com/zoobzio/herald"
    "github.com/zoobzio/herald/kafka"
)

type Order struct {
    ID           string `json:"id"`
    RetryCount   int    `json:"retry_count"`
}

var (
    orderReceived = capitan.NewSignal("order.received", "Order received")
    orderDLQ      = capitan.NewSignal("order.dlq", "Order dead-lettered")
    orderKey      = capitan.NewKey[Order]("order", "app.Order")
)

const maxRetries = 3

func main() {
    ctx := context.Background()

    // Main queue - create kafka-go reader
    mainReader := kafkago.NewReader(kafkago.ReaderConfig{
        Brokers: []string{"localhost:9092"},
        Topic:   "orders",
        GroupID: "order-processor",
    })
    defer mainReader.Close()

    mainProvider := kafka.New("orders", kafka.WithReader(mainReader))
    defer mainProvider.Close()

    mainSub := herald.NewSubscriber(mainProvider, orderReceived, orderKey, nil)
    mainSub.Start(ctx)
    defer mainSub.Close()

    // DLQ publisher - create kafka-go writer
    dlqWriter := &kafkago.Writer{
        Addr:  kafkago.TCP("localhost:9092"),
        Topic: "orders-dlq",
    }
    defer dlqWriter.Close()

    dlqProvider := kafka.New("orders-dlq", kafka.WithWriter(dlqWriter))
    defer dlqProvider.Close()

    dlqPub := herald.NewPublisher(dlqProvider, orderDLQ, orderKey, nil)
    dlqPub.Start(ctx)
    defer dlqPub.Close()

    // Processing with DLQ logic
    capitan.Hook(orderReceived, func(ctx context.Context, e *capitan.Event) {
        order, ok := orderKey.From(e)
        if !ok {
            return
        }

        err := processOrder(order)
        if err != nil {
            order.RetryCount++
            if order.RetryCount >= maxRetries {
                // Send to DLQ
                log.Error("Max retries exceeded, sending to DLQ", "order_id", order.ID)
                capitan.Emit(ctx, orderDLQ, orderKey.Field(order))
            } else {
                // Re-emit for retry (or let broker handle via Nack)
                log.Warn("Processing failed, will retry", "order_id", order.ID, "attempt", order.RetryCount)
                // Option: re-emit to same queue
                // capitan.Emit(ctx, orderReceived, orderKey.Field(order))
            }
        }
    })

    // DLQ handler (alerting, manual review, etc.)
    capitan.Hook(orderDLQ, func(ctx context.Context, e *capitan.Event) {
        order, _ := orderKey.From(e)
        log.Error("Order in DLQ", "order_id", order.ID)
        alertOncall(order)
    })

    select {}
}

func processOrder(o Order) error {
    // Processing logic that might fail
    return nil
}

func alertOncall(o Order) {
    // Send alert
}
```

## Validation Without Infinite Redelivery

Invalid messages should not cause infinite retry loops:

### Option 1: Validate and Drop

```go
validate := pipz.Apply("validate", func(ctx context.Context, o Order) (Order, error) {
    if o.ID == "" {
        // Log and return success to Ack the message
        log.Warn("Invalid order - no ID, dropping", "order", o)
        return o, nil // No error = Ack
    }
    if o.Total < 0 {
        log.Warn("Invalid order - negative total, dropping", "order", o)
        return o, nil
    }
    return o, nil
})

opts := []herald.Option[Order]{
    herald.WithPipeline[Order](validate),
}

sub := herald.NewSubscriber(provider, signal, key, opts)
```

### Option 2: Validate and Route to DLQ

```go
validate := pipz.Apply("validate", func(ctx context.Context, o Order) (Order, error) {
    if !isValid(o) {
        // Send to DLQ
        capitan.Emit(ctx, invalidOrderDLQ, orderKey.Field(o))
        return o, nil // Return success to Ack original
    }
    return o, nil
})
```

### Option 3: Validate in Handler

```go
capitan.Hook(signal, func(ctx context.Context, e *capitan.Event) {
    order, ok := key.From(e)
    if !ok {
        // Extraction failed - log and return (message Ack'd)
        log.Warn("Failed to extract order from event")
        return
    }

    if !isValid(order) {
        // Invalid - log and return (message Ack'd)
        log.Warn("Invalid order", "id", order.ID)
        return
    }

    // Valid - process
    if err := processOrder(order); err != nil {
        log.Error("Processing failed", "error", err)
        // Note: Error here doesn't affect Ack/Nack (already processed)
    }
})
```

## Circuit Breaker

Protect downstream services with circuit breaker:

```go
import "github.com/zoobzio/pipz"

// Create processor that calls external API
callAPI := pipz.Apply("api", func(ctx context.Context, o Order) (Order, error) {
    return externalService.Process(ctx, o)
})

// Wrap with circuit breaker
circuitBreaker := pipz.NewCircuitBreaker("external-api",
    callAPI,
    5,              // Open after 5 failures
    30*time.Second, // Wait 30s before half-open
)

opts := []herald.Option[Order]{
    herald.WithPipeline[Order](circuitBreaker),
}

pub := herald.NewPublisher(provider, signal, key, opts)
```

### Circuit Breaker States

```
       ┌──────────────────────────────────────────────────────┐
       │                                                       │
       │            success                 timeout            │
       │         ┌─────────┐            ┌───────────┐          │
       ▼         │         │            │           ▼          │
    ┌──────┐   ┌─┴─────────┴─┐     ┌────┴────┐    ┌────────┐  │
    │CLOSED│◄──│ HALF-OPEN   │────►│  OPEN   │────│ WAIT   │──┘
    └──────┘   └─────────────┘     └─────────┘    └────────┘
       │         failure            5 failures
       │
       └───────────────────────────────────────────────────────
                    normal operation
```

## Error Logging and Metrics

Track errors for observability:

```go
// Pipeline with error tracking
pipeline := pipz.NewSequence("order-pipeline",
    pipz.Apply("process", processOrder),
    pipz.Handle("errors",
        pipz.Apply("forward", forwardToNext),
        pipz.Effect("log-error", func(ctx context.Context, err error) error {
            log.Error("Pipeline error", "error", err)
            metrics.Inc("pipeline_errors")
            return err // Propagate error
        }),
    ),
)

opts := []herald.Option[Order]{
    herald.WithPipeline[Order](pipeline),
}
```

## Handler Failures vs Pipeline Failures

Understanding when Nack triggers redelivery:

```
Pipeline fails  → Nack() → Message redelivered
Pipeline succeeds → Ack() → Handler runs (async)
Handler fails   → Already ack'd → No redelivery
```

**Why?** Herald's responsibility ends at "deliver to capitan". By the time your handler runs, the message is already acknowledged. This is intentional - capitan handlers are async, there can be multiple, and herald can't know which one "owns" the message.

**If you need handler-level redelivery:**

```go
var (
    orderReceived = capitan.NewSignal("order.received", "Order from queue")
    orderRetry    = capitan.NewSignal("order.retry", "Order to retry")
)

capitan.Hook(orderReceived, func(ctx context.Context, e *capitan.Event) {
    order, ok := orderKey.From(e)
    if !ok {
        return
    }

    if err := processOrder(order); err != nil {
        // Handler failed - emit to retry queue
        log.Error("Processing failed, queueing retry", "error", err)
        capitan.Emit(ctx, orderRetry, orderKey.Field(order))
        return
    }
})
```

Or use a dedicated retry/DLQ publisher on the `orderRetry` signal.

## Graceful Degradation

Handle partial failures gracefully:

```go
capitan.Hook(orderReceived, func(ctx context.Context, e *capitan.Event) {
    order, ok := orderKey.From(e)
    if !ok {
        return
    }

    // Critical: must succeed
    if err := saveOrder(order); err != nil {
        log.Error("Failed to save order", "error", err)
        return // Note: message already ack'd at this point
    }

    // Best-effort: don't fail if these fail
    if err := sendConfirmation(order); err != nil {
        log.Warn("Failed to send confirmation", "error", err)
        // Continue - not critical
    }

    if err := updateAnalytics(order); err != nil {
        log.Warn("Failed to update analytics", "error", err)
        // Continue - not critical
    }
})
```

## Timeout Handling

Handle timeout scenarios:

```go
opts := []herald.Option[Order]{
    herald.WithTimeout[Order](5 * time.Second),
}

// In handler, check context
capitan.Hook(signal, func(ctx context.Context, e *capitan.Event) {
    select {
    case <-ctx.Done():
        log.Warn("Context cancelled", "error", ctx.Err())
        return
    default:
    }

    // Long-running operation with context
    if err := longOperation(ctx); err != nil {
        if errors.Is(err, context.DeadlineExceeded) {
            log.Warn("Operation timed out")
        } else if errors.Is(err, context.Canceled) {
            log.Info("Operation cancelled")
        } else {
            log.Error("Operation failed", "error", err)
        }
    }
})
```

## Idempotency

Handle duplicate messages:

```go
var processedIDs = sync.Map{} // Or use Redis/database

capitan.Hook(orderReceived, func(ctx context.Context, e *capitan.Event) {
    order, ok := orderKey.From(e)
    if !ok {
        return
    }

    // Check if already processed
    if _, exists := processedIDs.LoadOrStore(order.ID, true); exists {
        log.Info("Duplicate order, skipping", "id", order.ID)
        return
    }

    if err := processOrder(order); err != nil {
        // Remove from processed on failure (allow retry)
        processedIDs.Delete(order.ID)
        log.Error("Processing failed", "error", err)
    }
})
```

### Database-Backed Idempotency

```go
func processOrderIdempotent(ctx context.Context, order Order) error {
    tx, err := db.BeginTx(ctx, nil)
    if err != nil {
        return err
    }
    defer tx.Rollback()

    // Check if already processed
    var exists bool
    err = tx.QueryRow("SELECT EXISTS(SELECT 1 FROM processed_orders WHERE id = $1)", order.ID).Scan(&exists)
    if err != nil {
        return err
    }

    if exists {
        log.Info("Duplicate order", "id", order.ID)
        return nil // Already processed
    }

    // Process
    if err := doProcessOrder(tx, order); err != nil {
        return err
    }

    // Mark as processed
    _, err = tx.Exec("INSERT INTO processed_orders (id, processed_at) VALUES ($1, NOW())", order.ID)
    if err != nil {
        return err
    }

    return tx.Commit()
}
```

## Error Recovery Patterns

### Compensating Transaction

```go
capitan.Hook(orderReceived, func(ctx context.Context, e *capitan.Event) {
    order, _ := orderKey.From(e)

    // Step 1: Reserve inventory
    reservation, err := reserveInventory(order)
    if err != nil {
        log.Error("Failed to reserve inventory", "error", err)
        return
    }

    // Step 2: Charge payment
    payment, err := chargePayment(order)
    if err != nil {
        // Compensate: release inventory
        releaseInventory(reservation)
        log.Error("Failed to charge payment", "error", err)
        return
    }

    // Step 3: Create shipment
    _, err = createShipment(order)
    if err != nil {
        // Compensate: refund and release
        refundPayment(payment)
        releaseInventory(reservation)
        log.Error("Failed to create shipment", "error", err)
        return
    }

    log.Info("Order processed successfully", "id", order.ID)
})
```

## Best Practices

### 1. Distinguish Error Types

```go
type TransientError struct {
    Err error
}

type PermanentError struct {
    Err error
}

func processOrder(o Order) error {
    resp, err := api.Call(o)
    if err != nil {
        if isNetworkError(err) {
            return TransientError{err} // Retry
        }
        return PermanentError{err} // Don't retry
    }

    if resp.StatusCode >= 500 {
        return TransientError{fmt.Errorf("server error: %d", resp.StatusCode)}
    }

    if resp.StatusCode >= 400 {
        return PermanentError{fmt.Errorf("client error: %d", resp.StatusCode)}
    }

    return nil
}
```

### 2. Log Sufficient Context

```go
log.Error("Processing failed",
    "order_id", order.ID,
    "customer_id", order.CustomerID,
    "error", err,
    "correlation_id", meta["correlation-id"],
    "retry_count", retryCount,
)
```

### 3. Monitor Error Rates

```go
metrics.Inc("orders_processed", "status", "success")
metrics.Inc("orders_processed", "status", "error")
metrics.Inc("orders_retried")
metrics.Inc("orders_dead_lettered")
```

### 4. Alert on Error Thresholds

```go
errorRate := getErrorRate() // From metrics
if errorRate > 0.1 { // 10% error rate
    alert("High error rate in order processing")
}
```

## See Also

- [Pipeline Options](../3.guides/3.pipeline-options.md) - Timeout, backoff, rate limiting
- [Best Practices](../3.guides/5.best-practices.md) - Production patterns
- [pipz Documentation](https://github.com/zoobzio/pipz) - Advanced error handling
