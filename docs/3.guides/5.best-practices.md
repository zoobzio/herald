---
title: Best Practices
description: Production-ready patterns for herald integrations.
author: Herald Team
published: 2025-12-06
tags: [Guide, Best Practices, Production]
---

# Best Practices

Production-ready patterns for herald integrations.

## Signal Design

### One Direction Per Signal

A service should be either a Publisher OR Subscriber for a given signal, never both:

```go
// ✅ Service A: Publishes
pub := herald.NewPublisher(kafka, orderCreated, orderKey, nil)

// ✅ Service B: Subscribes
sub := herald.NewSubscriber(kafka, orderCreated, orderKey, nil)

// ❌ Same service does both (creates loop)
pub := herald.NewPublisher(kafka, orderCreated, orderKey, nil)
sub := herald.NewSubscriber(kafka, orderCreated, orderKey, nil)
```

### Use Different Signals for In/Out

Distinguish between signals for publishing vs receiving:

```go
var (
    // Signal for publishing (outbound)
    OrderCreated = capitan.NewSignal("order.created", "Order created by this service")

    // Signal for receiving (inbound)
    OrderReceived = capitan.NewSignal("order.received", "Order received from queue")
)
```

### Define Signals as Package Constants

```go
package events

import "github.com/zoobzio/capitan"

// Signals
var (
    OrderCreated   = capitan.NewSignal("order.created", "New order created")
    OrderShipped   = capitan.NewSignal("order.shipped", "Order has shipped")
    OrderDelivered = capitan.NewSignal("order.delivered", "Order delivered")
)

// Keys
var (
    OrderKey    = capitan.NewKey[Order]("order", "app.Order")
    ShipmentKey = capitan.NewKey[Shipment]("shipment", "app.Shipment")
)

// Types
type Order struct {
    ID         string    `json:"id"`
    CustomerID string    `json:"customer_id"`
    Total      float64   `json:"total"`
    CreatedAt  time.Time `json:"created_at"`
}
```

## Resource Management

### Proper Shutdown Order

The shutdown order matters for graceful termination:

```go
func main() {
    ctx, cancel := context.WithCancel(context.Background())

    // Create resources
    writer := createKafkaWriter()  // Your kafka-go writer
    defer writer.Close()

    provider := kafka.New("topic", kafka.WithWriter(writer))
    pub := herald.NewPublisher(provider, signal, key, nil)
    pub.Start(ctx)

    // Handle shutdown signal
    sigCh := make(chan os.Signal, 1)
    signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)
    <-sigCh

    // Shutdown in reverse order
    cancel()              // 1. Cancel context (stops new work)
    capitan.Shutdown()    // 2. Drain capitan queues
    pub.Close()           // 3. Close publisher
    provider.Close()      // 4. Close broker connection
}
```

**Why this order matters:**

1. **Cancel context** - Signals components to stop accepting new work
2. **capitan.Shutdown()** - Drains the event queue, ensuring all pending events reach their observers (including publishers)
3. **pub.Close() / sub.Close()** - Detaches from capitan and closes pipelines
4. **provider.Close()** - Closes broker connections

**What breaks with wrong order:**

- `pub.Close()` before `capitan.Shutdown()`: Events emitted but not yet processed by the publisher are lost - the observer is detached before it sees them
- `provider.Close()` before `pub.Close()`: Publish calls fail with connection errors
- Skipping `capitan.Shutdown()`: In-flight events may not complete processing

### Defer Cleanup

```go
func main() {
    writer := createKafkaWriter()  // Your kafka-go writer
    defer writer.Close()

    provider := kafka.New("topic", kafka.WithWriter(writer))
    defer provider.Close()

    pub := herald.NewPublisher(provider, signal, key, nil)
    defer pub.Close()

    pub.Start(ctx)
    defer capitan.Shutdown()

    // ... application logic ...
}
```

### Graceful Shutdown with Timeout

```go
func shutdown(pub *herald.Publisher[Order], provider herald.Provider) {
    // Give in-flight operations time to complete
    ctx, cancel := context.WithTimeout(context.Background(), 30*time.Second)
    defer cancel()

    // Close in order
    shutdownDone := make(chan struct{})
    go func() {
        capitan.Shutdown()
        pub.Close()
        provider.Close()
        close(shutdownDone)
    }()

    select {
    case <-shutdownDone:
        log.Info("Graceful shutdown complete")
    case <-ctx.Done():
        log.Warn("Shutdown timed out, forcing exit")
    }
}
```

## Error Handling

### Pipeline Options for Transient Errors

```go
opts := []herald.Option[Order]{
    // Timeout entire operation
    herald.WithTimeout[Order](30 * time.Second),

    // Retry transient failures
    herald.WithBackoff[Order](3, 500*time.Millisecond),
}

pub := herald.NewPublisher(provider, signal, key, opts)
```

### Subscriber Validation

Handle validation in a way that doesn't cause infinite redelivery:

```go
// Option 1: Validate in pipeline, drop invalid
validate := pipz.Apply("validate", func(ctx context.Context, o Order) (Order, error) {
    if o.ID == "" {
        log.Warn("Invalid order - no ID, dropping")
        return o, nil // Return nil error to Ack
    }
    return o, nil
})

opts := []herald.Option[Order]{
    herald.WithPipeline[Order](validate),
}

// Option 2: Validate in handler
capitan.Hook(signal, func(ctx context.Context, e *capitan.Event) {
    order, ok := key.From(e)
    if !ok {
        log.Warn("Failed to extract order")
        return // Already Ack'd at this point
    }

    if !isValid(order) {
        log.Warn("Invalid order", "id", order.ID)
        return // Log and drop
    }

    processOrder(order)
})
```

### Panic Recovery

Handlers should recover from panics:

```go
capitan.Hook(signal, func(ctx context.Context, e *capitan.Event) {
    defer func() {
        if r := recover(); r != nil {
            log.Error("Panic in handler", "error", r, "stack", debug.Stack())
            // Message already processed, don't crash
        }
    }()

    order, ok := key.From(e)
    if !ok {
        return
    }

    processOrder(order)
})
```

Note: capitan has built-in panic recovery, but explicit recovery allows custom handling.

## Observability

### Logging with Context

```go
capitan.Hook(signal, func(ctx context.Context, e *capitan.Event) {
    meta := herald.MetadataFromContext(ctx)
    correlationID := meta["correlation-id"]

    logger := log.With(
        "signal", e.Signal().Name(),
        "correlation-id", correlationID,
    )

    order, ok := key.From(e)
    if !ok {
        logger.Warn("Failed to extract order")
        return
    }

    logger.Info("Processing order", "order_id", order.ID)
    // ...
})
```

### Metrics

Track key metrics:

```go
var (
    messagesPublished = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "herald_messages_published_total",
            Help: "Total messages published",
        },
        []string{"signal", "status"},
    )

    messagesReceived = prometheus.NewCounterVec(
        prometheus.CounterOpts{
            Name: "herald_messages_received_total",
            Help: "Total messages received",
        },
        []string{"signal", "status"},
    )

    processingDuration = prometheus.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "herald_processing_duration_seconds",
            Help:    "Message processing duration",
            Buckets: prometheus.DefBuckets,
        },
        []string{"signal"},
    )
)
```

### With Shotel

Use shotel for automatic observability:

```go
import "github.com/zoobzio/shotel"

pvs, _ := shotel.DefaultProviders(ctx, "order-service", "v1.0.0", "localhost:4318")
defer pvs.Shutdown(ctx)

sh, _ := shotel.New(capitan.Default(), pvs.Log, pvs.Meter, pvs.Trace, &shotel.Config{
    Metrics: []shotel.MetricConfig{
        {Signal: events.OrderReceived, Name: "orders_received_total"},
        {Signal: events.ShipmentCreated, Name: "shipments_created_total"},
    },
})
defer sh.Close()
```

## Testing

### Use io Provider for Unit Tests

```go
func TestOrderProcessing(t *testing.T) {
    // Use io provider instead of real broker
    input := `{"id":"1","total":99.99}` + "\n"
    provider := heraldio.New(heraldio.WithReader(strings.NewReader(input)))

    // Use sync mode for deterministic tests
    c := capitan.New(capitan.WithSyncMode())
    defer c.Shutdown()

    // ... test logic ...
}
```

### Isolate Capitan Instances

```go
func TestA(t *testing.T) {
    c := capitan.New(capitan.WithSyncMode())
    defer c.Shutdown()

    pub := herald.NewPublisher(provider, signal, key, nil,
        herald.WithPublisherCapitan[Order](c),
    )
    // ...
}

func TestB(t *testing.T) {
    c := capitan.New(capitan.WithSyncMode())
    defer c.Shutdown()

    // Isolated from TestA
}
```

### Test Timeouts

Always use timeouts in tests:

```go
func TestWithTimeout(t *testing.T) {
    done := make(chan struct{})
    go func() {
        // ... test logic ...
        close(done)
    }()

    select {
    case <-done:
        // Success
    case <-time.After(5 * time.Second):
        t.Fatal("Test timed out")
    }
}
```

## Configuration

### Environment-Based Provider Selection

```go
func newProvider() herald.Provider {
    env := os.Getenv("ENVIRONMENT")

    switch env {
    case "production":
        // Create kafka-go reader with your configuration
        reader := kafkago.NewReader(kafkago.ReaderConfig{
            Brokers: strings.Split(os.Getenv("KAFKA_BROKERS"), ","),
            Topic:   "orders",
            GroupID: os.Getenv("KAFKA_GROUP_ID"),
        })
        return kafka.New("orders", kafka.WithReader(reader))
    case "staging":
        // Create SQS client implementing sqs.Client interface
        sqsClient := createSQSClient()
        return sqs.New(os.Getenv("SQS_QUEUE_URL"), sqs.WithClient(sqsClient))
    default:
        // Local development
        return bolt.New("orders", bolt.WithDB(openBoltDB()))
    }
}
```

### Configuration Struct

```go
type Config struct {
    Provider       string        `yaml:"provider"`
    KafkaBrokers   []string      `yaml:"kafka_brokers"`
    KafkaGroupID   string        `yaml:"kafka_group_id"`
    SQSQueueURL    string        `yaml:"sqs_queue_url"`
    Timeout        time.Duration `yaml:"timeout"`
    RetryAttempts  int           `yaml:"retry_attempts"`
    RetryDelay     time.Duration `yaml:"retry_delay"`
    RateLimit      float64       `yaml:"rate_limit"`
    RateBurst      int           `yaml:"rate_burst"`
}

func (c *Config) BuildOptions() []herald.Option[Order] {
    var opts []herald.Option[Order]

    if c.Timeout > 0 {
        opts = append(opts, herald.WithTimeout[Order](c.Timeout))
    }

    if c.RetryAttempts > 0 {
        opts = append(opts, herald.WithBackoff[Order](c.RetryAttempts, c.RetryDelay))
    }

    if c.RateLimit > 0 {
        opts = append(opts, herald.WithRateLimit[Order](c.RateLimit, c.RateBurst))
    }

    return opts
}
```

## Performance

### Batch Publishing (Manual)

For high-throughput scenarios, batch before publishing:

```go
type BatchPublisher struct {
    pub      *herald.Publisher[[]Order]
    buffer   []Order
    mu       sync.Mutex
    maxSize  int
    interval time.Duration
}

func (b *BatchPublisher) Add(order Order) {
    b.mu.Lock()
    b.buffer = append(b.buffer, order)
    if len(b.buffer) >= b.maxSize {
        b.flush()
    }
    b.mu.Unlock()
}

func (b *BatchPublisher) flush() {
    if len(b.buffer) == 0 {
        return
    }
    batch := b.buffer
    b.buffer = nil
    capitan.Emit(context.Background(), batchSignal, batchKey.Field(batch))
}
```

### Rate Limiting Subscribers

Prevent overwhelming downstream systems:

```go
opts := []herald.Option[Order]{
    herald.WithRateLimit[Order](100, 20), // 100/sec, burst 20
}

sub := herald.NewSubscriber(provider, signal, key, opts)
```

### Avoid Expensive Operations in Handlers

```go
// ❌ Bad: Expensive operation per message
capitan.Hook(signal, func(ctx context.Context, e *capitan.Event) {
    order, _ := key.From(e)
    result := expensiveComputation(order) // Blocks processing
    // ...
})

// ✅ Good: Offload expensive work
capitan.Hook(signal, func(ctx context.Context, e *capitan.Event) {
    order, _ := key.From(e)
    workQueue <- order // Fast, non-blocking
})

// Separate goroutine pool handles expensive work
for order := range workQueue {
    result := expensiveComputation(order)
    // ...
}
```

## Security

### Sanitize Metadata

Don't trust incoming metadata blindly:

```go
capitan.Hook(signal, func(ctx context.Context, e *capitan.Event) {
    meta := herald.MetadataFromContext(ctx)

    // Sanitize before using
    correlationID := sanitizeID(meta["correlation-id"])
    if correlationID == "" {
        correlationID = generateID()
    }

    // Don't use metadata in SQL queries without parameterization
    // Don't log sensitive metadata values
})

func sanitizeID(id string) string {
    // Allow only alphanumeric and hyphens
    if !regexp.MustCompile(`^[a-zA-Z0-9-]+$`).MatchString(id) {
        return ""
    }
    if len(id) > 64 {
        return ""
    }
    return id
}
```

### Don't Publish Sensitive Data

```go
type Order struct {
    ID         string  `json:"id"`
    CustomerID string  `json:"customer_id"`
    Total      float64 `json:"total"`
    // Don't include:
    // CreditCard string `json:"credit_card"` // Never
    // Password   string `json:"password"`    // Never
}
```

## Summary Checklist

- [ ] One direction per signal (publish OR subscribe)
- [ ] Signals defined as package constants
- [ ] Proper shutdown order
- [ ] Pipeline options for reliability
- [ ] Validation that doesn't cause infinite redelivery
- [ ] Panic recovery in handlers
- [ ] Correlation IDs for tracing
- [ ] Metrics and logging
- [ ] io provider for unit tests
- [ ] Isolated capitan instances in tests
- [ ] Environment-based configuration
- [ ] Rate limiting for high-throughput
- [ ] Sanitized metadata

## Next Steps

- [Testing Guide](./1.testing.md) - Comprehensive testing patterns
- [Error Handling Cookbook](../4.cookbook/3.error-handling.md) - Advanced error patterns
- [Architecture](../1.learn/3.architecture.md) - Understanding internals
