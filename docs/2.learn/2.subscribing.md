---
title: Subscribing
description: Consume broker messages as capitan events
author: zoobzio
published: 2025-12-11
updated: 2025-12-11
tags:
  - Learn
  - Subscribing
---

# Subscribing

Subscribers consume messages from external brokers and emit them as capitan events.

## Basic Usage

```go
package main

import (
    "context"
    "fmt"

    kafkago "github.com/segmentio/kafka-go"
    "github.com/zoobzio/capitan"
    "github.com/zoobzio/herald"
    "github.com/zoobzio/herald/pkg/kafka"
)

type Order struct {
    ID    string  `json:"id"`
    Total float64 `json:"total"`
}

func main() {
    ctx := context.Background()

    // Define signal and key
    orderCreated := capitan.NewSignal("order.created", "New order placed")
    orderKey := capitan.NewKey[Order]("order", "app.Order")

    // Create provider
    reader := kafkago.NewReader(kafkago.ReaderConfig{
        Brokers: []string{"localhost:9092"},
        Topic:   "orders",
        GroupID: "order-processor",
    })
    provider := kafka.New("orders", kafka.WithReader(reader))
    defer provider.Close()

    // Hook listener before starting subscriber
    capitan.Hook(orderCreated, func(ctx context.Context, e *capitan.Event) {
        order, ok := orderKey.From(e)
        if ok {
            fmt.Printf("Received order: %s ($%.2f)\n", order.ID, order.Total)
        }
    })

    // Create and start subscriber
    sub := herald.NewSubscriber(provider, orderCreated, orderKey, nil)
    sub.Start()
    defer sub.Close()

    // Block until shutdown
    select {}
}
```

## How It Works

1. **Subscriber starts consuming** — When `sub.Start()` is called, the subscriber spawns a goroutine that calls `provider.Subscribe(ctx)`.

2. **Message received** — The provider returns messages through a channel.

3. **Deserialize** — The subscriber unmarshals the message bytes into the typed value using the codec.

4. **Attach metadata** — Message metadata is attached to the context.

5. **Emit to capitan** — The typed value is emitted to capitan on the specified signal.

6. **Acknowledge** — On successful emission, the message is acknowledged. On failure, it's nack'd for redelivery.

## Subscriber Options

### Pipeline Options

Add reliability features:

```go
sub := herald.NewSubscriber(provider, signal, key, []herald.Option[Order]{
    herald.WithTimeout[Order](5*time.Second),
    herald.WithRateLimit[Order](100, 10),
})
```

### Custom Codec

Use a different serialization format:

```go
sub := herald.NewSubscriber(provider, signal, key, nil,
    herald.WithSubscriberCodec[Order](myProtobufCodec))
```

### Custom Capitan Instance

Use an isolated capitan instance:

```go
c := capitan.New()
sub := herald.NewSubscriber(provider, signal, key, nil,
    herald.WithSubscriberCapitan[Order](c))
```

## Metadata Access

Message metadata is available in listener context:

```go
capitan.Hook(orderCreated, func(ctx context.Context, e *capitan.Event) {
    meta := herald.MetadataFromContext(ctx)
    correlationID := meta["correlation-id"]
    traceID := meta["trace-id"]

    order, _ := orderKey.From(e)
    processOrder(ctx, order, correlationID)
})
```

## Acknowledgment Semantics

Herald handles acknowledgment automatically:

| Outcome | Action |
|---------|--------|
| Successful deserialization + emit | `Ack()` called |
| Deserialization failure | `Nack()` called |
| Pipeline failure | `Nack()` called |

Each provider implements broker-appropriate ack/nack behavior:

| Provider | Ack | Nack |
|----------|-----|------|
| Kafka | Commit offset | Don't commit (redelivered) |
| JetStream | `msg.Ack()` | `msg.Nak()` (redelivered) |
| Pub/Sub | `msg.Ack()` | `msg.Nack()` |
| Redis | `XACK` | Remains pending |
| SQS | Delete message | Returns after visibility timeout |
| AMQP | `Ack(false)` | `Nack(false, true)` with requeue |

## Error Handling

Deserialization and processing errors are emitted as capitan events:

```go
capitan.Hook(herald.ErrorSignal, func(ctx context.Context, e *capitan.Event) {
    err, _ := herald.ErrorKey.From(e)

    switch err.Operation {
    case "unmarshal":
        log.Printf("Failed to deserialize message: %v", err.Err)
        log.Printf("Raw payload: %s", err.Raw)
    case "subscribe":
        log.Printf("Subscription error: %v", err.Err)
    case "ack":
        log.Printf("Failed to acknowledge: %v", err.Err)
    case "nack":
        log.Printf("Failed to nack: %v", err.Err)
    }
})
```

## Lifecycle

```go
// Create subscriber
sub := herald.NewSubscriber(provider, signal, key, nil)

// Start consuming (spawns goroutine)
ctx, cancel := context.WithCancel(context.Background())
sub.Start()

// ... process messages ...

// Stop consuming
cancel()      // Signal goroutine to stop
sub.Close()   // Wait for cleanup
```

## Graceful Shutdown

The subscriber responds to context cancellation:

```go
ctx, cancel := context.WithCancel(context.Background())
sub.Start()

// On shutdown signal
cancel()                  // Stop consuming
sub.Close()              // Wait for in-flight messages
capitan.Shutdown()       // Drain capitan queue
provider.Close()         // Close broker connection
```

## Best Practices

### Hook Listeners Before Starting

Register capitan listeners before starting the subscriber to avoid missing events:

```go
// First: set up listeners
capitan.Hook(signal, handler)

// Then: start subscriber
sub.Start()
```

### Use Consumer Groups

For scalable consumption, use broker consumer groups:

```go
// Kafka
reader := kafkago.NewReader(kafkago.ReaderConfig{
    GroupID: "order-processor",  // Consumer group
    // ...
})

// Redis
provider := redis.New("stream", redis.WithGroup("processor-group"))
```

### Handle Poison Messages

Messages that consistently fail deserialization will be nack'd repeatedly. Consider:
- Dead letter queues at the broker level
- Error signal handlers that log problematic payloads
- Circuit breakers to pause consumption on repeated failures
